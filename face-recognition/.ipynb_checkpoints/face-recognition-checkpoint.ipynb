{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08145f30-86d4-48ee-bbd5-9deb3a9e0ded",
   "metadata": {},
   "source": [
    "## Collection of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc220d5-1ad9-4a85-9a8a-90a4c4fa612d",
   "metadata": {},
   "source": [
    "This program uses the Open CV (Open Computer Vision)library to capture all the sample data (images) using the webcame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf347add-4728-4d6e-9266-ed49c37416ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: \"is\" with 'tuple' literal. Did you mean \"==\"?\n",
      "<>:15: SyntaxWarning: \"is\" with 'tuple' literal. Did you mean \"==\"?\n",
      "C:\\Users\\arsar\\AppData\\Local\\Temp\\ipykernel_1264\\2787068834.py:15: SyntaxWarning: \"is\" with 'tuple' literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face not found\n",
      "Face not found\n",
      "Face found\n",
      "Face found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face not found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face not found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face not found\n",
      "Face not found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Face found\n",
      "Collecting Samples Complete\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(img, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        x=x-10\n",
    "        y=y-10\n",
    "        cropped_face = img[y:y+h+50, x:x+w+50]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (400, 400))\n",
    "        #face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Save file in specified directory with unique name\n",
    "        file_name_path = './Images/' + str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        print(\"Face found\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77003c8b-b4d8-42a2-a20d-bda8e293a68a",
   "metadata": {},
   "source": [
    "Below program we are using the pretrained model VGG model and making the model trainable as false to not to retrain them, then we are doing a softmax prediction to find out the classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea8846-b93b-4a6f-8cea-b5b7cc13f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Krish.Naik\n",
    "\"\"\"\n",
    "\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# re-size all the images to this\n",
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "train_path = 'Datasets/Train'\n",
    "valid_path = 'Datasets/Test'\n",
    "\n",
    "# add preprocessing layer to the front of VGG\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "# don't train existing weights\n",
    "for layer in vgg.layers:\n",
    "  layer.trainable = False\n",
    "  \n",
    "\n",
    "  \n",
    "  # useful for getting number of classes\n",
    "folders = glob('Datasets/Train/*')\n",
    "  \n",
    "\n",
    "# our layers - you can add more if you want\n",
    "x = Flatten()(vgg.output)\n",
    "# x = Dense(1000, activation='relu')(x)\n",
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "# view the structure of the model\n",
    "model.summary()\n",
    "\n",
    "# tell the model what cost and optimization method to use\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('Datasets/Train',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('Datasets/Test',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "'''r=model.fit_generator(training_set,\n",
    "                         samples_per_epoch = 8000,\n",
    "                         nb_epoch = 5,\n",
    "                         validation_data = test_set,\n",
    "                         nb_val_samples = 2000)'''\n",
    "\n",
    "# fit the model\n",
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=5,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")\n",
    "# loss\n",
    "plt.plot(r.history['loss'], label='train loss')\n",
    "plt.plot(r.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('LossVal_loss')\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='train acc')\n",
    "plt.plot(r.history['val_acc'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('AccVal_acc')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('facefeatures_new_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5460001-d342-40e9-a34a-015bc3674a67",
   "metadata": {},
   "source": [
    "Below code is used to get the image as the input and fit into the classification ANN model to predict the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021af5c-f28e-4f31-82c6-33d4a328705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jan 15 14:49:19 2019\n",
    "\n",
    "@author: krish.naik\n",
    "\"\"\"\n",
    "\n",
    "# Face Recognition\n",
    "\n",
    "# Importing the libraries\n",
    "from PIL import Image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import image\n",
    "model = load_model('facefeatures_new_model_final.h5')\n",
    "\n",
    "# Loading the cascades\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Doing some Face Recognition with the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    _, frame = video_capture.read()\n",
    "    #canvas = detect(gray, frame)\n",
    "    #image, face =face_detector(frame)\n",
    "    \n",
    "    face=face_extractor(frame)\n",
    "    if type(face) is np.ndarray:\n",
    "        face = cv2.resize(face, (224, 224))\n",
    "        im = Image.fromarray(face, 'RGB')\n",
    "           #Resizing into 128x128 because we trained the model with this image size.\n",
    "        img_array = np.array(im)\n",
    "                    #Our keras model used a 4D tensor, (images x height x width x channel)\n",
    "                    #So changing dimension 128x128x3 into 1x128x128x3 \n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        pred = model.predict(img_array)\n",
    "        print(pred)\n",
    "                     \n",
    "        name=\"None matching\"\n",
    "        \n",
    "        if(pred[0][3]>0.5):\n",
    "            name='Krish'\n",
    "        cv2.putText(frame,name, (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame,\"No face found\", (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
